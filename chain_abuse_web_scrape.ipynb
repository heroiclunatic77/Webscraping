{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "import csv\n",
        "\n",
        "# Base URL for the reports\n",
        "base_url = \"https://www.chainabuse.com/_next/data/J8xBrAMAZD8pw5GgZq_Cf/en/reports.json?page={page_number}\"\n",
        "\n",
        "# Prepare to save data in CSV format\n",
        "csv_filename = \"report_details.csv\"\n",
        "csv_columns = [\"Description\", \"Reported By\", \"Scam Category\", \"Report ID\", \"Source\"]\n",
        "\n",
        "# Open the CSV file in write mode\n",
        "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=csv_columns)\n",
        "    writer.writeheader()  # Write the header row\n",
        "\n",
        "    # Loop through the first 100 pages\n",
        "    for page_number in range(2):  # Pages 0 to 99\n",
        "        # Construct the full URL for the current page\n",
        "        url = base_url.format(page_number=page_number)\n",
        "\n",
        "        # Fetch JSON data from the current page\n",
        "        response = requests.get(url)\n",
        "        text_data = response.text  # Get the response as text\n",
        "\n",
        "        # Regular expression to find report IDs\n",
        "        report_ids = re.findall(r'\"Report\",\"id\":\"(.*?)\"', text_data)\n",
        "\n",
        "        # Base URL for fetching report details\n",
        "        report_base_url = \"https://www.chainabuse.com/_next/data/J8xBrAMAZD8pw5GgZq_Cf/en/report/{report_id}.json?context=browse-chain&chain=BTC&reportId={report_id}\"\n",
        "\n",
        "        # Loop through each report ID\n",
        "        for report_id in report_ids:\n",
        "            # Construct the full URL for the report\n",
        "            report_url = report_base_url.format(report_id=report_id)\n",
        "\n",
        "            # Fetch the report data\n",
        "            report_response = requests.get(report_url)\n",
        "\n",
        "            if report_response.status_code == 200:\n",
        "                # Parse the JSON data from the response\n",
        "                report_data = report_response.json()\n",
        "\n",
        "                # Extract the report details from the 'pageProps' field\n",
        "                report_info = report_data.get('pageProps', {})\n",
        "\n",
        "                # Extracting required values\n",
        "                description = report_info.get(\"description\", \"No description found\")\n",
        "                reported_by = report_info.get(\"reportedBy\", \"No reportedBy found\")\n",
        "                scam_category = report_info.get(\"scamCategory\", \"No scamCategory found\")\n",
        "                report_id_value = report_data.get('pageProps', {}).get('initialApolloState', {}).get(f'Report:{report_id}', {}).get('id', \"No report ID found\")\n",
        "\n",
        "                # Extract the source from the initialApolloState field\n",
        "                source = report_data.get('pageProps', {}).get('initialApolloState', {}).get(f'Report:{report_id}', {}).get('source', \"No source found\")\n",
        "\n",
        "                # Writing the extracted values to the CSV file\n",
        "                writer.writerow({\n",
        "                    \"Description\": description,\n",
        "                    \"Reported By\": reported_by,\n",
        "                    \"Scam Category\": scam_category,\n",
        "                    \"Report ID\": report_id_value,\n",
        "                    \"Source\": source  # Add source dynamically\n",
        "                })\n",
        "            else:\n",
        "                print(f\"Failed to fetch details for the report with ID: {report_id}\")\n",
        "\n",
        "        print(f\"Finished processing page {page_number + 1} of 100\")\n",
        "\n",
        "print(f\"Data has been saved to {csv_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRCuECLXLmjb",
        "outputId": "2fcce717-24da-4c37-93dd-7cd75f473a6b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished processing page 1 of 100\n",
            "Finished processing page 2 of 100\n",
            "Data has been saved to report_details.csv\n"
          ]
        }
      ]
    }
  ]
}